{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pay Attention\n",
    "This notebook dives into the implementation of Self Attention mechanism. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](media/self_attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding the Input\n",
    "First, let's convert the sentence into something actually \"learnable\".  \n",
    "Start by creating the dictionary of tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'brown': 1, 'dog': 2, 'fox': 3, 'jumps': 4, 'lazy': 5, 'over': 6, 'quick': 7, 'the': 8}\n"
     ]
    }
   ],
   "source": [
    "sentence = 'the quick brown fox jumps over a lazy dog'\n",
    "\n",
    "dc = {s:i for i,s in enumerate(sorted(sentence.replace(',', '').split()))}\n",
    "print(dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert the sentence to integer-vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8, 7, 1, 3, 4, 6, 0, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "sentence_int = torch.tensor([dc[s] for s in sentence.replace(',', '').split()])\n",
    "print(sentence_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the vector dense: use an embedding layer to encode the inputs into a real-vector embedding.  \n",
    " Here, we will use a 16-dimensional embedding such that each input word is represented by a 16-dimensional vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.8338e-01, -4.1891e-01, -8.0483e-01,  5.6561e-01,  6.1036e-01,\n",
      "          4.6688e-01,  1.9507e+00, -1.0631e+00, -7.7326e-02,  1.1640e-01,\n",
      "         -5.9399e-01, -1.2439e+00, -1.0209e-01, -1.0335e+00, -3.1264e-01,\n",
      "          2.4579e-01],\n",
      "        [-2.1883e-01, -2.4351e+00, -7.2915e-02, -3.3987e-02,  9.6252e-01,\n",
      "          3.4917e-01, -9.2146e-01, -5.6195e-02, -6.2270e-01, -4.6372e-01,\n",
      "          1.9218e+00, -4.0255e-01,  1.2390e-01,  1.1648e+00,  9.2337e-01,\n",
      "          1.3873e+00],\n",
      "        [-1.3527e+00, -1.6959e+00,  5.6665e-01,  7.9351e-01,  5.9884e-01,\n",
      "         -1.5551e+00, -3.4136e-01,  1.8530e+00,  7.5019e-01, -5.8550e-01,\n",
      "         -1.7340e-01,  1.8348e-01,  1.3894e+00,  1.5863e+00,  9.4630e-01,\n",
      "         -8.4368e-01],\n",
      "        [-6.7309e-01,  8.7283e-01,  1.0554e+00,  1.7784e-01, -2.3034e-01,\n",
      "         -3.9175e-01,  5.4329e-01, -3.9516e-01, -4.4622e-01,  7.4402e-01,\n",
      "          1.5210e+00,  3.4105e+00, -1.5312e+00, -1.2341e+00,  1.8197e+00,\n",
      "         -5.5153e-01],\n",
      "        [-5.6925e-01,  9.1997e-01,  1.1108e+00,  1.2899e+00, -1.4782e+00,\n",
      "          2.5672e+00, -4.7312e-01,  3.3555e-01, -1.6293e+00, -5.4974e-01,\n",
      "         -4.7983e-01, -4.9968e-01, -1.0670e+00,  1.1149e+00, -1.4067e-01,\n",
      "          8.0575e-01],\n",
      "        [ 1.6459e+00, -1.3602e+00,  3.4457e-01,  5.1987e-01, -2.6133e+00,\n",
      "         -1.6965e+00, -2.2824e-01,  2.7995e-01,  2.4693e-01,  7.6887e-02,\n",
      "          3.3801e-01,  4.5440e-01,  4.5694e-01, -8.6537e-01,  7.8131e-01,\n",
      "         -9.2679e-01],\n",
      "        [-1.1258e+00, -1.1524e+00, -2.5058e-01, -4.3388e-01,  8.4871e-01,\n",
      "          6.9201e-01, -3.1601e-01, -2.1152e+00,  3.2227e-01, -1.2633e+00,\n",
      "          3.4998e-01,  3.0813e-01,  1.1984e-01,  1.2377e+00,  1.1168e+00,\n",
      "         -2.4728e-01],\n",
      "        [-9.3348e-02,  6.8705e-01, -8.3832e-01,  8.9182e-04,  8.4189e-01,\n",
      "         -4.0003e-01,  1.0395e+00,  3.5815e-01, -2.4600e-01,  2.3025e+00,\n",
      "         -1.8817e+00, -4.9727e-02, -1.0450e+00, -9.5650e-01,  3.3532e-02,\n",
      "          7.1009e-01],\n",
      "        [-6.1358e-01,  3.1593e-02, -4.9268e-01,  2.4841e-01,  4.3970e-01,\n",
      "          1.1241e-01,  6.4079e-01,  4.4116e-01, -1.0231e-01,  7.9244e-01,\n",
      "         -2.8967e-01,  5.2507e-02,  5.2286e-01,  2.3022e+00, -1.4689e+00,\n",
      "         -1.5867e+00]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "embed = torch.nn.Embedding(len(sentence.replace(',','').split()), 16)\n",
    "embedded_sentence = embed(sentence_int).detach()\n",
    "print(embedded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Weights Matrices:   \n",
    "- Weights Query\n",
    "- Weights Keys\n",
    "- Weights Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "d = embedded_sentence.shape[1]\n",
    "d_q, d_k, d_v = 24, 24, 28\n",
    "\n",
    "W_q = nn.Parameter(torch.randn(d_q, d))\n",
    "W_k = nn.Parameter(torch.randn(d_k, d))\n",
    "W_v = nn.Parameter(torch.randn(d_v, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using matrix multiplication we do obtain the respective Query, Key and Values sequences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the unnormalized attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input word for which we want to compute the attention is: 'brown' with dictionary index 1 and embedding \n",
      "tensor([-0.2188, -2.4351, -0.0729, -0.0340,  0.9625,  0.3492, -0.9215, -0.0562,\n",
      "        -0.6227, -0.4637,  1.9218, -0.4025,  0.1239,  1.1648,  0.9234,  1.3873])\n",
      "\n",
      "Query: tensor([ -2.1911,  -1.4461,   0.4942,  -0.9959,  -4.7663,  -4.8887,   9.3072,\n",
      "          5.8991,  -3.8935,  -4.2999,  -1.3099,  -1.1042, -13.4091,  -2.6684,\n",
      "         -1.8868,  -1.6594,   4.1670,  -4.1148,   0.0935,   9.3019,  -1.6530,\n",
      "          2.6375,  -5.8936,  -0.0373], grad_fn=<MvBackward0>)\n",
      "Key: tensor([ 7.7921, -3.4727,  3.3231,  0.8337,  7.1973,  2.5318, -3.6451,  4.7367,\n",
      "        -6.4336,  0.5191, -8.1601, -8.5292,  1.0385, -2.9421,  3.7129, -0.4032,\n",
      "         7.9608,  8.9526, -0.7115, -2.3812, -1.3893,  3.9426, -0.2862,  0.7675],\n",
      "       grad_fn=<MvBackward0>)\n",
      "Value: tensor([-0.3520, -3.7708, -3.8933, -4.7494, -3.1310, -2.4028,  4.3370,  0.2605,\n",
      "        -1.4421, -2.1825, -7.1633, -3.5063,  2.2980,  2.6982, -4.3681,  7.9956,\n",
      "         1.6718,  0.9460,  0.9402,  2.5976, -0.4163, -3.8349, -5.7704, -2.7109,\n",
      "         5.7989,  5.9459, -0.7766, -1.0017], grad_fn=<MvBackward0>)\n",
      "\n",
      "Query shape: torch.Size([24])\n",
      "Key shape: torch.Size([24])\n",
      "Value shape: torch.Size([28])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(0, len(sentence_int), (1,)).item()\n",
    "\n",
    "print(f\"The input word for which we want to compute the attention is: '{list(dc.keys())[list(dc.values()).index(x)]}' with dictionary index {x} and embedding \\n{embedded_sentence[x]}\")\n",
    "\n",
    "x = embedded_sentence[x]\n",
    "query_x = W_q @ x\n",
    "key_x = W_k @ x\n",
    "value_x = W_v @ x\n",
    "print(f\"\\nQuery: {query_x}\\nKey: {key_x}\\nValue: {value_x}\")\n",
    "print(f\"\\nQuery shape: {query_x.shape}\\nKey shape: {key_x.shape}\\nValue shape: {value_x.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the unnormalized self-attention weights we have to compute the remaining key and values for all inputs as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keys shape: torch.Size([9, 24])\n",
      "Values shape: torch.Size([9, 28])\n"
     ]
    }
   ],
   "source": [
    "keys = (W_k @ embedded_sentence.T).T\n",
    "values = (W_v @ embedded_sentence.T).T\n",
    "print(f\"\\nKeys shape: {keys.shape}\\nValues shape: {values.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the unnormalized attention weights: $\\omega_{i,j}=q^{(i)^T}k^{(j)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Omega_x: tensor([  32.4191,  -44.9209,   30.4341, -121.9629,  -61.8859, -123.4025,\n",
      "         171.9292,    4.6947,   79.1044], grad_fn=<SqueezeBackward4>)\n",
      "\n",
      "Omega_x shape: torch.Size([9])\n"
     ]
    }
   ],
   "source": [
    "omega_x = query_x @ keys.T\n",
    "print(f\"\\nOmega_x: {omega_x}\")\n",
    "print(f\"\\nOmega_x shape: {omega_x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the respective normalized Attention Scores, first scale by $\\frac{1}{\\sqrt{d_k}}$(ensures that the Euclidean length of the weight vectors will be approximately in the same magnitude) and then apply the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention weights for the input word: tensor([4.2897e-13, 5.9736e-20, 2.8606e-13, 8.8403e-27, 1.8719e-21, 6.5893e-27,\n",
      "        1.0000e+00, 1.4951e-15, 5.9031e-09], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Attention weights shape: torch.Size([9])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "attention_weights_x = F.softmax(omega_x/d_k**0.5, dim=0)\n",
    "print(f\"\\nAttention weights for the input word: {attention_weights_x}\")\n",
    "print(f\"\\nAttention weights shape: {attention_weights_x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the last step is to compute the context vector $z^{(x)}$, which is an attention-weighted version of our original query input, including all the other input elements as its context via the attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context vector for the input word: tensor([-6.1658,  3.3317, -1.4784,  3.0280, -3.0778, -1.9382,  3.2093,  2.9632,\n",
      "         4.7867,  2.5697, -1.9187, -0.8907,  3.5392, -0.1726, -2.6539,  5.6142,\n",
      "        -1.1907,  2.2681, -6.4134,  2.0330,  3.2004, -8.4279, -5.9757, -6.8775,\n",
      "         3.2998,  4.7060, -3.5087,  5.1399], grad_fn=<SqueezeBackward4>)\n",
      "\n",
      "Context vector shape: torch.Size([28])\n"
     ]
    }
   ],
   "source": [
    "context_vector_x = attention_weights_x @ values\n",
    "print(f\"\\nContext vector for the input word: {context_vector_x}\")\n",
    "print(f\"\\nContext vector shape: {context_vector_x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multihead query weight shape: torch.Size([3, 24, 16]), \n",
      "Multihead key weight shape: torch.Size([3, 24, 16]), \n",
      "Multihead value weight shape: torch.Size([3, 28, 16])\n"
     ]
    }
   ],
   "source": [
    "heads = 3\n",
    "multihead_W_q = nn.Parameter(torch.randn(heads, d_q, d))\n",
    "multihead_W_k = nn.Parameter(torch.randn(heads, d_k, d))\n",
    "multihead_W_v = nn.Parameter(torch.randn(heads, d_v, d))\n",
    "print(f\"\\nMultihead query weight shape: {multihead_W_q.shape}, \\nMultihead key weight shape: {multihead_W_k.shape}, \\nMultihead value weight shape: {multihead_W_v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multihead query shape: torch.Size([3, 24])\n"
     ]
    }
   ],
   "source": [
    "multihead_query_x = multihead_W_q @ x\n",
    "print(f\"\\nMultihead query shape: {multihead_query_x.shape}\")\n",
    "multihead_key_x = multihead_W_k @ x\n",
    "multihead_value_x = multihead_W_v @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacked inputs shape: torch.Size([3, 16, 9])\n"
     ]
    }
   ],
   "source": [
    "stacked_inputs = embedded_sentence.T.repeat(heads, 1, 1)\n",
    "print(f\"\\nStacked inputs shape: {stacked_inputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_keys.shape: torch.Size([3, 24, 9])\n",
      "multihead_values.shape: torch.Size([3, 28, 9])\n"
     ]
    }
   ],
   "source": [
    "multihead_keys = torch.bmm(multihead_W_k, stacked_inputs)\n",
    "multihead_values = torch.bmm(multihead_W_v, stacked_inputs)\n",
    "print(\"multihead_keys.shape:\", multihead_keys.shape)\n",
    "print(\"multihead_values.shape:\", multihead_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_keys.shape: torch.Size([3, 9, 24])\n",
      "multihead_values.shape: torch.Size([3, 9, 28])\n"
     ]
    }
   ],
   "source": [
    "multihead_keys = multihead_keys.permute(0, 2, 1)\n",
    "multihead_values = multihead_values.permute(0, 2, 1)\n",
    "print(\"multihead_keys.shape:\", multihead_keys.shape)\n",
    "print(\"multihead_values.shape:\", multihead_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Omega_multihead_x shape: torch.Size([3, 9])\n"
     ]
    }
   ],
   "source": [
    "omega_multihead_x = torch.bmm(multihead_query_x.unsqueeze(1), multihead_keys.transpose(1,2)).squeeze(1)\n",
    "print(f\"\\nOmega_multihead_x shape: {omega_multihead_x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multihead attention weights shape: torch.Size([3, 9])\n"
     ]
    }
   ],
   "source": [
    "multihead_attention_weights_x = F.softmax(omega_multihead_x/d_k**0.5, dim=1)\n",
    "print(f'Multihead attention weights shape: {multihead_attention_weights_x.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multihead context vector shape: torch.Size([3, 28])\n"
     ]
    }
   ],
   "source": [
    "multihead_context_vector_x = torch.bmm(multihead_attention_weights_x.unsqueeze(1), multihead_values).squeeze(1)\n",
    "print(f'Multihead context vector shape: {multihead_context_vector_x.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Attention\n",
    "In self-attention, we work with the same input sequence. In cross-attention, we mix or combine two different input sequences.\n",
    "![Alt text](media/cross-attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keys shape: torch.Size([8, 24])\n",
      "Values shape: torch.Size([8, 28])\n"
     ]
    }
   ],
   "source": [
    "embedded_sentence_2 = torch.rand(8,16)\n",
    "keys = (W_k @ embedded_sentence_2.T).T\n",
    "values = (W_v @ embedded_sentence_2.T).T\n",
    "print(f\"\\nKeys shape: {keys.shape}\\nValues shape: {values.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that compared to self-attention, the keys and values now have 8 instead of 6 rows. Everything else stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Omega_x shape: torch.Size([8])\n",
      "\n",
      " Attention weights shape: torch.Size([8])\n",
      "\n",
      " Context vector shape: torch.Size([28])\n"
     ]
    }
   ],
   "source": [
    "omega_x = query_x @ keys.T\n",
    "print(f'\\n Omega_x shape: {omega_x.shape}')\n",
    "attention_weights_x = F.softmax(omega_x/d_k**0.5, dim=0)\n",
    "print(f'\\n Attention weights shape: {attention_weights_x.shape}')\n",
    "context_vector_x = attention_weights_x @ values\n",
    "print(f'\\n Context vector shape: {context_vector_x.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
